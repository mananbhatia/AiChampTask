{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os   \n",
    "from glob import glob\n",
    "#library to read text from pdfs\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#gives us the current working directory\n",
    "curr_directory = os.getcwd()\n",
    "#print(curr_directory)\n",
    "#gives the directory where all pdfs are stored\n",
    "profile_path = os.path.join(curr_directory + \"\\\\profile_pdfs\\\\\")\n",
    "#print(profile_path)\n",
    "list_pdf_profile_paths = glob(os.path.join(profile_path,\"*.{}\".format('pdf')))\n",
    "#print(list_pdf_profile_paths[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 1 Complete and Task 2 start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading text from pdf files\n",
    "def extractTextFromPDF(filepath):\n",
    "    doc = fitz.open(filepath)                      # open document\n",
    "    #doc object of fitz stores the document page objects\n",
    "    all_text = \"\"\n",
    "    for page in doc:\n",
    "        #print(page.getText()) \n",
    "        #page.getText() returns the text contained in a single page\n",
    "        all_text = all_text  + page.getText()\n",
    "\n",
    "    return all_text\n",
    "    #print(all_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching text from all profiles in a list\n",
    "text_all_profiles = [extractTextFromPDF(path) for path in list_pdf_profile_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profiles = pd.DataFrame()\n",
    "df_profiles['SNo'] = [i for i in range(1,51)]\n",
    "df_profiles['Profile_Text'] = text_all_profiles\n",
    "df_profiles.head()\n",
    "df_profiles.to_csv(\"Output\\\\submission_Task2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 2 COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import operator\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        #print(wordnet.ADJ)\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAndTokenizeText(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    en_stop = stopwords.words('english')\n",
    "    #this extendes the list of stopwords \n",
    "    en_stop.extend(['www','gmail','month','page','linkedin','com','india','year'])\n",
    "    months = ['january','february','march','april','may','june','july','august','september','october','november','december']\n",
    "    en_stop.extend(months)\n",
    "    en_stop = set(en_stop)    \n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stopped_tokens = [j for j in tokens if not j in en_stop]\n",
    "    stopped_tokens = [j for j in stopped_tokens if len(j)>1]\n",
    "    pos_tokens = pos_tag(stopped_tokens)\n",
    "    tokens_final = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for word in pos_tokens:\n",
    "        try:\n",
    "            lem_word = wordnet_lemmatizer.lemmatize(word[0], pos=get_wordnet_pos(word[1]))\n",
    "            #print (\"{0:20}{1:20}\".format(word[0],lem_word))\n",
    "            tokens_final.append(lem_word)\n",
    "        except:\n",
    "            #tokens_final.append(word[0])    #uncomment if you want to include numbers and unknown words\n",
    "            pass\n",
    "            #to handle errors where words are digits or words which are not known in English Language or other errors\n",
    "\n",
    "    tokens_final = [j for j in tokens_final if not j in en_stop]\n",
    "    return tokens_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostFrequentWords(text):\n",
    "    list_words = cleanAndTokenizeText(text)\n",
    "    freq_words = {}\n",
    "    for word in list_words:\n",
    "        if(word not in freq_words):\n",
    "            freq_words[word] = 1\n",
    "        else:\n",
    "            freq_words[word]+=1 \n",
    "\n",
    "    sorted_freq_words =  sorted(freq_words.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    most_frequent_words = [i[0] for i in sorted_freq_words]\n",
    "    most_frequent_words = most_frequent_words[0:20]\n",
    "    return most_frequent_words\n",
    "    #print(\"Top 20 words freuency wise are:\",sorted_freq_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profiles['Most_Frequent_Words'] = df_profiles['Profile_Text'].apply(lambda x:getMostFrequentWords(x))\n",
    "df_profiles.to_csv(\"Output\\\\submission_Task3_mostFreqWords.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostImpWords(list_of_documents_text):\n",
    "    \n",
    "    bloblist = [cleanAndTokenizeText(blob) for blob in list_of_documents_text]\n",
    "    bloblist = [\" \".join(i for i in blob) for blob in bloblist]\n",
    "    bloblist = [tb(blob) for blob in bloblist]\n",
    "    essential_words = []\n",
    "    for i, blob in enumerate(bloblist):\n",
    "        #print(\"Top words in document {}\".format(i + 1))\n",
    "        scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "        sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        imp_words = []\n",
    "        for word, score in sorted_words[:10]:\n",
    "            imp_words.append(word)\n",
    "            #print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
    "        essential_words.append(imp_words)\n",
    "\n",
    "    return essential_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_documents_text = list(df_profiles['Profile_Text'])\n",
    "df_profiles['Most Important Words'] = getMostImpWords(list_of_documents_text)\n",
    "df_profiles.to_csv(\"Output\\Submission_Task3_MostImpWords.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough work ,understanding, practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text = text_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing and removing special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#\\w+ captures characters,numbers and underscore and removes all special charcters as well as spaces\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#create English stop words list\n",
    "en_stop = stopwords.words('english')\n",
    "#this extendes the list of stopwords \n",
    "en_stop.extend(['www','gmail','month','page','linkedin','com','india'])\n",
    "months = ['january','february','march','april','may','june','july','august','september','october','november','december']\n",
    "en_stop.extend(months)\n",
    "#this makes the list unique\n",
    "en_stop = set(en_stop)    \n",
    "text = text.lower()\n",
    "#this splits the string into tokens or words\n",
    "tokens = tokenizer.tokenize(text)\n",
    "#print(tokens)\n",
    "print(len(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# remove stop words from tokens\n",
    "stopped_tokens = [j for j in tokens if not j in en_stop]\n",
    "#remove one character words from tokens    \n",
    "stopped_tokens = [j for j in stopped_tokens if len(j)>1]\n",
    "#print(stopped_tokens)\n",
    "print(len(stopped_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing the words to its root form based on parts of speech tagging and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "pos_tokens = pos_tag(stopped_tokens)\n",
    "print(pos_tokens[0:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokens_final = []\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word in pos_tokens:\n",
    "    try:\n",
    "        lem_word = wordnet_lemmatizer.lemmatize(word[0], pos=get_wordnet_pos(word[1]))\n",
    "        #print (\"{0:20}{1:20}\".format(word[0],lem_word))\n",
    "        tokens_final.append(lem_word)\n",
    "    except:\n",
    "        #tokens_final.append(word[0])\n",
    "        pass\n",
    "        #print(\"\\n!!Error here:\",word[0],\"!!\\n\") \n",
    "        #to handle errors where words are digits or words which are not known in English Language\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#just to confirm removal if any new stop_words were introduced are lemmatization\n",
    "tokens_final = [j for j in tokens_final if not j in en_stop]\n",
    "len(tokens_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "freq_words = {}\n",
    "for word in tokens_final:\n",
    "    if(word not in freq_words):\n",
    "        freq_words[word] = 1\n",
    "    else:\n",
    "        freq_words[word]+=1 \n",
    "\n",
    "sorted_freq_words =  sorted(freq_words.items(), key=operator.itemgetter(1),reverse=True)\n",
    "print(\"Top 20 words frequency wise are:\",sorted_freq_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bloblist = list(df_profiles['Profile_Text'])\n",
    "bloblist = [cleanAndTokenizeText(blob) for blob in bloblist ]\n",
    "bloblist = [\" \".join(i for i in blob) for blob in bloblist]\n",
    "bloblist = [tb(blob) for blob in bloblist]\n",
    "#bloblist[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "bloblist = text.split(\"\\n\")\n",
    "bloblist = [cleanAndTokenizeText(blob) for blob in bloblist ]\n",
    "bloblist = [\" \".join(i for i in blob) for blob in bloblist]\n",
    "bloblist = [blob for blob in bloblist if blob!='']\n",
    "bloblist = [tb(blob) for blob in bloblist]\n",
    "print(bloblist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "essential_words = []\n",
    "for i, blob in enumerate(bloblist):\n",
    "    #print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    imp_words = []\n",
    "    for word, score in sorted_words[:10]:\n",
    "        imp_words.append(word)\n",
    "        #print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
    "    essential_words.append(imp_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "scores_all = {}\n",
    "\n",
    "for i, blob in enumerate(bloblist):\n",
    "    #print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    scores_all.update(scores)\n",
    "\n",
    "sorted_words = sorted(scores_all.items(), key=lambda x: x[1], reverse=True)\n",
    "for word, score in sorted_words[:10]:\n",
    "    print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  }
 ]
}
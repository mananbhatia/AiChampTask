{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os   \n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library to read text from pdfs\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "m:\\AiChamps\n"
    }
   ],
   "source": [
    "#gives us the current working directory\n",
    "curr_directory = os.getcwd()\n",
    "print(curr_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "m:\\AiChamps\\profile_pdfs\\\n"
    }
   ],
   "source": [
    "#gives the directory where all pdfs are stored\n",
    "profile_path = os.path.join(curr_directory + \"\\\\profile_pdfs\\\\\")\n",
    "print(profile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['m:\\\\AiChamps\\\\profile_pdfs\\\\Profile (1).pdf', 'm:\\\\AiChamps\\\\profile_pdfs\\\\Profile (10).pdf', 'm:\\\\AiChamps\\\\profile_pdfs\\\\Profile (11).pdf', 'm:\\\\AiChamps\\\\profile_pdfs\\\\Profile (12).pdf', 'm:\\\\AiChamps\\\\profile_pdfs\\\\Profile (13).pdf']\n"
    }
   ],
   "source": [
    "list_pdf_profile_paths = glob(os.path.join(profile_path,\"*.{}\".format('pdf')))\n",
    "print(list_pdf_profile_paths[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 1 Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 2 Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading text from pdf files\n",
    "def extractTextFromPDF(filepath):\n",
    "    doc = fitz.open(filepath)                      # open document\n",
    "    #doc object of fitz stores the document page objects\n",
    "    all_text = \"\"\n",
    "    for page in doc:\n",
    "        #print(page.getText()) \n",
    "        #page.getText() returns the text contained in a single page\n",
    "        all_text = all_text  + page.getText()\n",
    "\n",
    "    return all_text\n",
    "    #print(all_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_profile_path = list_pdf_profile_paths[0] \n",
    "text_test = extractTextFromPDF(test_profile_path)\n",
    "#print(text_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching text from all profiles in a list\n",
    "text_all_profiles = [extractTextFromPDF(path) for path in list_pdf_profile_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profiles = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profiles['SNo'] = [i for i in range(1,51)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profiles['Profile_Text'] = text_all_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   SNo                                       Profile_Text\n0    1   \\n \\nContact\\nbhatia.manan96@gmail.com\\nwww.l...\n1    2   \\n \\nContact\\nbhatiasahil96@gmail.com\\nwww.li...\n2    3   \\n \\nContact\\nakashpandey111@gmail.com\\nwww.l...\n3    4   \\n \\nContact\\nNoida Sector 125\\n9719207080 (M...\n4    5   \\n \\nContact\\nyashup1997@gmail.com\\nwww.linke...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SNo</th>\n      <th>Profile_Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>\\n \\nContact\\nbhatia.manan96@gmail.com\\nwww.l...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>\\n \\nContact\\nbhatiasahil96@gmail.com\\nwww.li...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>\\n \\nContact\\nakashpandey111@gmail.com\\nwww.l...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>\\n \\nContact\\nNoida Sector 125\\n9719207080 (M...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>\\n \\nContact\\nyashup1997@gmail.com\\nwww.linke...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "source": [
    "df_profiles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(df_profiles['Profile_Text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profiles.to_csv(\"Output\\\\submission_Task2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 2 COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import operator\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rough work and explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing and removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "417\n"
    }
   ],
   "source": [
    "#\\w+ captures characters,numbers and underscore and removes all special charcters as well as spaces\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# create English stop words list\n",
    "en_stop = stopwords.words('english')\n",
    "#this extendes the list of stopwords \n",
    "en_stop.extend(['www','gmail','month','page','linkedin','com','india'])\n",
    "months = ['january','february','march','april','may','june','july','august','september','october','november','december']\n",
    "en_stop.extend(months)\n",
    "#this makes the list unique\n",
    "en_stop = set(en_stop)    \n",
    "text = text.lower()\n",
    "#this splits the string into tokens or words\n",
    "tokens = tokenizer.tokenize(text)\n",
    "#print(tokens)\n",
    "print(len(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "278\n"
    }
   ],
   "source": [
    "# remove stop words from tokens\n",
    "stopped_tokens = [j for j in tokens if not j in en_stop]\n",
    "#remove one character words from tokens    \n",
    "stopped_tokens = [j for j in stopped_tokens if len(j)>1]\n",
    "#print(stopped_tokens)\n",
    "print(len(stopped_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing the words to its root form based on parts of speech tagging and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('contact', 'NN'), ('bhatia', 'NN'), ('manan96', 'NN'), ('manan', 'NN'), ('bhatia', 'NN'), ('top', 'JJ'), ('skills', 'NNS'), ('python', 'VBP'), ('robotic', 'JJ'), ('process', 'NN'), ('automation', 'NN'), ('rpa', 'NN'), ('google', 'NN'), ('cloud', 'NN'), ('api', 'NN'), ('certifications', 'NNS'), ('intro', 'VBP'), ('python', 'NN'), ('data', 'NNS'), ('science', 'NN')]\n"
    }
   ],
   "source": [
    "pos_tokens = pos_tag(stopped_tokens)\n",
    "print(pos_tokens[0:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        #print(wordnet.ADJ)\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens_final = []\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word in pos_tokens:\n",
    "    try:\n",
    "        lem_word = wordnet_lemmatizer.lemmatize(word[0], pos=get_wordnet_pos(word[1]))\n",
    "        #print (\"{0:20}{1:20}\".format(word[0],lem_word))\n",
    "        tokens_final.append(lem_word)\n",
    "    except:\n",
    "        #tokens_final.append(word[0])\n",
    "        pass\n",
    "        #print(\"\\n!!Error here:\",word[0],\"!!\\n\") \n",
    "        #to handle errors where words are digits or words which are not known in English Language\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "259"
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "source": [
    "#just to confirm removal if any new stop_words were introduced are lemmatization\n",
    "tokens_final = [j for j in tokens_final if not j in en_stop]\n",
    "len(tokens_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Top 20 words frequency wise are: [('process', 11), ('data', 11), ('document', 8), ('analytics', 7), ('information', 7), ('automation', 6), ('python', 5), ('extraction', 4), ('time', 4), ('bhatia', 3), ('api', 3), ('transorg', 3), ('analyst', 3), ('automate', 3), ('extract', 3), ('manan', 2), ('robotic', 2), ('rpa', 2), ('google', 2), ('cloud', 2)]\n"
    }
   ],
   "source": [
    "freq_words = {}\n",
    "for word in tokens_final:\n",
    "    if(word not in freq_words):\n",
    "        freq_words[word] = 1\n",
    "    else:\n",
    "        freq_words[word]+=1 \n",
    "\n",
    "sorted_freq_words =  sorted(freq_words.items(), key=operator.itemgetter(1),reverse=True)\n",
    "print(\"Top 20 words frequency wise are:\",sorted_freq_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing it all Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAndTokenizeText(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    en_stop = stopwords.words('english')\n",
    "    #this extendes the list of stopwords \n",
    "    en_stop.extend(['www','gmail','month','page','linkedin','com','india','year'])\n",
    "    months = ['january','february','march','april','may','june','july','august','september','october','november','december']\n",
    "    en_stop.extend(months)\n",
    "    en_stop = set(en_stop)    \n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stopped_tokens = [j for j in tokens if not j in en_stop]\n",
    "    stopped_tokens = [j for j in stopped_tokens if len(j)>1]\n",
    "    pos_tokens = pos_tag(stopped_tokens)\n",
    "    tokens_final = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for word in pos_tokens:\n",
    "        try:\n",
    "            lem_word = wordnet_lemmatizer.lemmatize(word[0], pos=get_wordnet_pos(word[1]))\n",
    "            #print (\"{0:20}{1:20}\".format(word[0],lem_word))\n",
    "            tokens_final.append(lem_word)\n",
    "        except:\n",
    "            #tokens_final.append(word[0])    #uncomment if you want to include numbers and unknown words\n",
    "            pass\n",
    "            #to handle errors where words are digits or words which are not known in English Language or other errors\n",
    "\n",
    "    tokens_final = [j for j in tokens_final if not j in en_stop]\n",
    "    return tokens_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostFrequentWords(text):\n",
    "    list_words = cleanAndTokenizeText(text)\n",
    "    freq_words = {}\n",
    "    for word in list_words:\n",
    "        if(word not in freq_words):\n",
    "            freq_words[word] = 1\n",
    "        else:\n",
    "            freq_words[word]+=1 \n",
    "\n",
    "    sorted_freq_words =  sorted(freq_words.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    most_frequent_words = [i[0] for i in sorted_freq_words]\n",
    "    most_frequent_words = most_frequent_words[0:20]\n",
    "    return most_frequent_words\n",
    "    #print(\"Top 20 words freuency wise are:\",sorted_freq_words[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profiles['Most_Frequent_Words'] = df_profiles['Profile_Text'].apply(lambda x:getMostFrequentWords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profiles.to_csv(\"Output\\\\submission_Task3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}